{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyshark\n",
    "import random\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from json import JSONDecoder\n",
    "from json import loads\n",
    "from json import dumps\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, check_output\n",
    "\n",
    "# Extract funtion is used to flatten nested dictionaries to a Lvl 1 Dictionary\n",
    "# Cons: It does not avoid collisions\n",
    "def extract(dict_in, dict_out):\n",
    "    for key, value in dict_in.iteritems():\n",
    "        if isinstance(value, dict): # If value itself is dictionary\n",
    "            extract(value, dict_out)\n",
    "        elif isinstance(value, unicode):\n",
    "            # Write to dict_out\n",
    "            dict_out[key] = value\n",
    "    return dict_out\n",
    "\n",
    "# Parse Object Pairs function identifies all keys that are equal at the same level\n",
    "# and calls make_unique to rename the keyname\n",
    "def parse_object_pairs(pairs):\n",
    "    dct = OrderedDict()\n",
    "    #dct = dict()\n",
    "    for key, value in pairs:\n",
    "        if key in dct:\n",
    "            key = make_unique(key, dct)\n",
    "        dct[key] = value\n",
    "    return dct\n",
    "\n",
    "# Make Unique function is in charge of renaming keys which have the same name \n",
    "def make_unique(key, dct):\n",
    "    counter = 0\n",
    "    unique_key = key\n",
    "\n",
    "    while unique_key in dct:\n",
    "        counter += 1\n",
    "        unique_key = '{}-{}'.format(key, str(counter).zfill(2))\n",
    "    return unique_key\n",
    "\n",
    "\n",
    "# Parse Object Pairs function identifies all keys that are equal at the same level\n",
    "# and calls make_unique to rename the keyname\n",
    "def parse_object_pairs2(pairs):\n",
    "    dct = OrderedDict()\n",
    "    dummy_dct = OrderedDict()\n",
    "    #print(\"PAIRS:\")\n",
    "    #print(pairs)\n",
    "    #print(\"\\n\")\n",
    "    for key, value in pairs:\n",
    "        #print(\"Make Unique Loop\")\n",
    "        key = make_unique2(key, dct, dummy_dct)\n",
    "        dct[key] = value\n",
    "    return dct\n",
    "\n",
    "# Make Unique function is in charge of renaming keys which have the same name \n",
    "def make_unique2(key, dct, dummy_dct):\n",
    "    counter = 0\n",
    "    unique_key = key\n",
    "    #print(\"Counter %s\" %counter)\n",
    "    #print(\"Unique Key:\")\n",
    "    #print(unique_key)\n",
    "    #print(\"Dictionary\")\n",
    "    #print(dct)\n",
    "    if unique_key not in dummy_dct:\n",
    "        #print(\"*** IF Loop ***\")\n",
    "        #print(dct)\n",
    "        #print(\"Pre-Unique: %s\" %unique_key)\n",
    "        unique_key = '{}-{}'.format(key, str(counter).zfill(2))\n",
    "        #print(\"Pos-Unique: %s\" %unique_key)\n",
    "        #print(\"*** Dummy DCT ***\")\n",
    "        dummy_dct[key] = \"\"\n",
    "        #print(dummy_dct)\n",
    "    else:\n",
    "        while unique_key in dct or unique_key in dummy_dct:\n",
    "            #print(\"*** Else Loop ***\")\n",
    "            #print(dct)\n",
    "            #print(\"Pre-Unique: %s\" %unique_key)\n",
    "            unique_key = '{}-{}'.format(key, str(counter).zfill(2))\n",
    "            #print(\"Pos-Unique: %s\" %unique_key)\n",
    "            counter += 1\n",
    "    #print(\"\\n\")\n",
    "    return unique_key\n",
    "\n",
    "# Flatten DIct function is in charge of flattening the nested dictionaries\n",
    "# While flattening, it will rename the keys at the last level by prepending predecesor key names\n",
    "def flatten_dict(d, prefix='>'):\n",
    "    def items():\n",
    "        # A clojure for recursively extracting dict like values\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                for sub_key, sub_value in flatten_dict(value).items():\n",
    "                    # Key name should imply nested origin of the dict,\n",
    "                    # so we use a default prefix of __ instead of _ or .\n",
    "                    if key == \"_source-00\" or key == \"layers-00\":\n",
    "                        yield sub_key, sub_value\n",
    "                    else:\n",
    "                        yield key + prefix + sub_key, sub_value\n",
    "            else:\n",
    "                yield key, value\n",
    "    return dict(items())\n",
    "\n",
    "# Define the Main Directory containing all uncompressed PCAPS\n",
    "pcaps_dir='/home/cc/Contagio'\n",
    "# Create a list of the Sub-directories within the Main Directory\n",
    "directories = [x[0] for x in os.walk(pcaps_dir)]\n",
    "\n",
    "# Print list of Sub-directories\n",
    "print(directories)\n",
    "uncpx_pcaps_dir = directories\n",
    "\n",
    "# Define required empty lists\n",
    "failed_pcaps=[]\n",
    "\n",
    "# Define variables\n",
    "mw_id_counter = 0\n",
    "\n",
    "# Create List of Unique IDs to identify to assign to each Malware PCAP\n",
    "mw_random_ids = sorted(random.sample(range(10000,20000), 1300))\n",
    "#mw_id = mw_random_ids[0]\n",
    "\n",
    "# Dictionary for matching Malware ID and Malware Name\n",
    "mw_id_dict = {}\n",
    "\n",
    "#Create Empty Dataframe\n",
    "columns = ['mw_id']\n",
    "df_ = pd.DataFrame(index=[0], columns=columns)\n",
    "\n",
    "# The following loop will perform the following actions:\n",
    "# 1) Loop through every Sub-directory\n",
    "# 2) Loop through every PCAP within each Sub-directory\n",
    "# 3) Read every PCAP using tshark filtering only frames containing ssl traffic\n",
    "#    In addtion, the output of all protocol features will be delivered in a nested JSON format\n",
    "# 4) The extracted dictionary will be parsed using parse_object_pairs and flatten_dict function\n",
    "for pcap_dir in uncpx_pcaps_dir[0:1]: #uncpx_pcaps_dir[0:1]\n",
    "    pcap_dir = \"/home/cc/Contagio/Exploit_Kits_and_Malware\"\n",
    "    print(\"\\n\")\n",
    "    print(pcap_dir)\n",
    "    pcap_dir_files = [f for f in listdir(pcap_dir) if isfile(join(pcap_dir, f))]\n",
    "    print(\"\\n\")\n",
    "    pcap_dir_files = [\"EK_MALWARE_2014-04-29-fake-flash-updater-02_mailware-traffic-analysis.net.pcap\"]\n",
    "    for i_cap in pcap_dir_files:\n",
    "        # Get a new malware ID\n",
    "        mw_id = mw_random_ids[mw_id_counter]\n",
    "        # Save Malware Id and Name to Dictionary\n",
    "        mw_id_dict[mw_id] = i_cap\n",
    "        print(\"ID: %s\" %mw_id)\n",
    "        print(\"MW: %s\" %mw_id_dict[mw_id])\n",
    "        # Increase Counter\n",
    "        mw_id_counter = mw_id_counter + 1\n",
    "        # Print PCAP\n",
    "        print(i_cap)\n",
    "        # Executed Tshark Commands to Filter frames containing SSL in PCAP\n",
    "        command = [\"tshark\", \"-n\", \"-r\", \"{}\".format(pcap_dir+'/'+i_cap), \"-Y\", \"ssl\", \"-T\", \"fields\", \"-e\", \"frame.number\"]\n",
    "        #command = [\"tshark\", \"-n\", \"-r\", \"{}\".format(pcap_dir+'/'+i_cap), \"-Y\", \"ssl\", \"-T\", \"json\"]\n",
    "        #command = [\"tshark\", \"-n\", \"-r\", \"{}\".format(pcap_dir+'/'+i_cap), \"-2\", \"-R\", \"ssl\", \"-Y\", \"frame.number==1\", \"-T\", \"json\" ]\n",
    "        process = Popen(command, universal_newlines=True, stdout=PIPE, stderr=PIPE)\n",
    "        output, err = process.communicate(b\"input data that is passed to subprocess' stdin\")\n",
    "        rc = process.returncode\n",
    "        # Print Number of Packets in SSL Frame\n",
    "        #packet_numbers = [int(x) for x in output[:-1].split('\\n')]\n",
    "        packet_numbers = list(map(int, re.findall(r'\\d+', output)))\n",
    "        print(\"Number of Frames containing SSL: %s\" %len(packet_numbers))\n",
    "        print(\"\\n\")\n",
    "        # Loop for extracting and parsing data per frame on PCAP\n",
    "        for p_num in packet_numbers:\n",
    "            command = [\"tshark\", \"-n\", \"-r\", \"{}\".format(pcap_dir+'/'+i_cap), \"-Y\", \"frame.number=={}\".format(p_num), \"-T\", \"json\" ]\n",
    "            process2 = Popen(command, universal_newlines=True, stdout=PIPE, stderr=PIPE)\n",
    "            output2, err2 = process2.communicate(b\"input data that is passed to subprocess' stdin\")\n",
    "            decoder = JSONDecoder(object_pairs_hook=parse_object_pairs2)\n",
    "            user_dict = decoder.decode(output2[2:-4])\n",
    "            # Flatten received dictionary\n",
    "            user_dict_final = flatten_dict(user_dict)\n",
    "            # Add Malware ID to Temp Dataframe\n",
    "            user_dict_final[\"mw_id\"] = mw_id\n",
    "            # Create Temp Dataframe\n",
    "            df = pd.DataFrame(user_dict_final, index=[1])\n",
    "            # Append Temp Dataframe to Main Dataframe (df_)\n",
    "            #df_ = pd.concat([df_,df])\n",
    "            df_ = df_.append([df], ignore_index=True)\n",
    "\n",
    "print(\"\\nFailed PCAPS\")\n",
    "print(failed_pcaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list of discovered features\n",
    "total_features = list(df_.columns.values)\n",
    "print(len(total_features))\n",
    "pprint(total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mw_id_series\n",
    "mw_id_series = pd.Series(mw_id_dict)\n",
    "# Save mw_id_series\n",
    "mw_id_series.to_pickle('mw_id_series.p')\n",
    "# Save SSL df (df_)\n",
    "df_.to_pickle('mw_ssl_dataframe.p')\n",
    "# Save SSL features list\n",
    "ssl_features_file = open('ssl_features.txt', 'w')\n",
    "for item in total_features:\n",
    "    ssl_features_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pretty function walks a dictionary and prints it based with specific number of spaces per level\n",
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('    ' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('    ' * (indent+1) + str(value))\n",
    "            \n",
    "# SortOD function read a dictionary and sorts keys in alphabetical order per level          \n",
    "def sortOD(od):\n",
    "    res = OrderedDict()\n",
    "    for k, v in sorted(od.items()):\n",
    "        if isinstance(v, dict):\n",
    "            res[k] = sortOD(v)\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return res\n",
    "\n",
    "# Variable for creating an Ordered Dictionary for Features\n",
    "features_tree = OrderedDict()\n",
    "\n",
    "# Read the list of features and transfer them to an ordered dictionary\n",
    "for feature_item in total_features:\n",
    "    t = features_tree\n",
    "    for part in feature_item.split('>'):\n",
    "        t = t.setdefault(part, {})\n",
    "\n",
    "# Sort features in alphabetical order\n",
    "features_tree_ordered = sortOD(features_tree)\n",
    "# Print Ordered Dictionary in Pretty Mode\n",
    "pretty(features_tree_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "## Tshark\n",
    "https://ask.wireshark.org/questions/58343/2-pass-filter-in-wiresharktshark\n",
    "https://hackertarget.com/tshark-tutorial-and-filter-examples/\n",
    "https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html\n",
    "http://www.thegeekstuff.com/2012/07/wireshark-filter/?ref=driverlayer\n",
    "\n",
    "## Pandas and JSON\n",
    "https://www.dataquest.io/blog/python-json-tutorial/\n",
    "https://stackoverflow.com/questions/21104592/json-to-pandas-dataframe\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "https://stackoverflow.com/questions/23019119/converting-multilevel-nested-dictionaries-to-pandas-dataframe\n",
    "http://pbpython.com/pandas-list-dict.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\n",
    "https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\n",
    "https://pandas.pydata.org/pandas-docs/stable/options.html\n",
    "http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/\n",
    "https://www.digitalocean.com/community/tutorials/an-introduction-to-the-pandas-package-and-its-data-structures-in-python-3\n",
    "\n",
    "## Unique Dictionary Keys\n",
    "https://stackoverflow.com/questions/29321677/python-json-parser-allow-duplicate-keys\n",
    "https://stackoverflow.com/questions/36086470/parsing-python-json-with-multiple-same-strings-with-different-values\n",
    "https://stackoverflow.com/questions/29321677/python-json-parser-allow-duplicate-keys/29323197\n",
    "\n",
    "## Flattening Dictionaries\n",
    "https://stackoverflow.com/questions/12507206/python-recommended-way-to-walk-complex-dictionary-structures-imported-from-json\n",
    "https://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys\n",
    "\n",
    "## Zip Files\n",
    "https://unix.stackexchange.com/questions/270555/how-to-get-number-of-files-in-a-7z"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
